{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c7448b",
   "metadata": {},
   "source": [
    "### Data-cleaning setup\n",
    "\n",
    "We begin the cleaning workflow by importing **pandas**, the core library we’ll use for loading, inspecting, and transforming the accident dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b6efe",
   "metadata": {},
   "source": [
    "### Import required library\n",
    "\n",
    "We load **pandas**, the main Python toolkit for data wrangling and analysis, and give it the conventional alias `pd` for shorter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4a057",
   "metadata": {},
   "source": [
    "### Load the aviation accident dataset\n",
    "\n",
    "We start by importing the dataset, which includes all recorded aviation accidents from 1962 to 2023.\n",
    "The file isn’t encoded in UTF-8, so trying to load it that way causes an error.\n",
    "Using `encoding=\"latin1\"` instead solves the problem and lets us read in the full dataset.\n",
    "We then use `df.head()` to take a quick look at the first few rows and get a sense of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AviationData.csv\", encoding = \"latin1\") # utf-8 throws an error, so switched to latin1 encoding\n",
    "df.head() # initial data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80a382",
   "metadata": {},
   "source": [
    "### Check dataset dimensions and structure\n",
    "\n",
    "Before cleaning, it helps to know how large the dataset is and what it contains.\n",
    "The code below:\n",
    "\n",
    "i. Prints the **number of rows** and **columns** so we see the overall size.\n",
    "ii. Calls `df.info()` to show basic data types and missing-value counts.\n",
    "iii. Lists all column names, giving us a clear inventory of the fields we’ll be working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c1668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to understand how big the data we're working with is\n",
    "tuple_shape = df.shape\n",
    "\n",
    "print(f\"Rows: {tuple_shape[0]}\")\n",
    "print(f\"Columns: {tuple_shape[1]}\")\n",
    "df.info()\n",
    "print(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9df151",
   "metadata": {},
   "source": [
    "### Convert date columns to true datetime objects\n",
    "\n",
    "Turning the text-based date columns into pandas `datetime` format lets us later sort, filter, or group by year and month.\n",
    "Using `errors=\"coerce\"` quietly sets any bad or empty strings to `NaT` (pandas’ “not-a-time” value), so the loop finishes without crashing even if some dates are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert dates to datetime format (not sure if needed, just for practise)\n",
    "date_cols = [\"Event.Date\", \"Publication.Date\"]\n",
    "\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ca319",
   "metadata": {},
   "source": [
    "### Convert key count columns to numeric\n",
    "\n",
    "These five columns hold numbers (engine count and injury totals) but were read in as text.\n",
    "`pd.to_numeric(..., errors=\"coerce\")` changes them to proper numeric types and turns any bad entries into `NaN`, which we can handle later.\n",
    "A quick `dtypes` printout confirms the conversion worked for both the numeric and date columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, convert numeric columns to Numeric values\n",
    "numeric_cols = [\"Number.of.Engines\", \"Total.Fatal.Injuries\", \"Total.Serious.Injuries\", \"Total.Minor.Injuries\", \"Total.Uninjured\"]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors =\"coerce\")\n",
    "\n",
    "# Make sure our conversion worked (it did)\n",
    "print(df[numeric_cols].dtypes)\n",
    "print(df[date_cols].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c862ca",
   "metadata": {},
   "source": [
    "### Remove columns that aren’t useful for our risk analysis\n",
    "\n",
    "The fields listed in `irrelevant_columns` are mostly IDs, location codes, or carrier details that won’t help us judge aircraft safety.\n",
    "We drop them to keep the dataset focused and easier to work with, then call `df_simple.info()` to confirm the new, slimmer structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to drop colums that won't help our analysis\n",
    "irrelevant_columns = [\"Event.Id\", \"Accident.Number\", \"Latitude\", \"Longitude\", \"Airport.Code\", \"Airport.Name\", \"Registration.Number\", \"Schedule\", \"Location\", \"Air.carrier\", \"Report.Status\", \"Country\"]\n",
    "df_simple = df.drop(columns=irrelevant_columns)\n",
    "df_simple.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a514644",
   "metadata": {},
   "source": [
    "### Fill missing numbers with each column’s median value\n",
    "\n",
    "To avoid losing rows that have a few missing injury or engine counts, we fill those gaps with the median of each column.\n",
    "The median is a safe choice because it isn’t thrown off by extreme accident records.\n",
    "After filling, we call `df_simple.info()` again to confirm there are no remaining nulls in the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to fill the NaN in the numerical columns by entering the median\n",
    "for i in numeric_cols:\n",
    "    df_simple[i] = df_simple[i].fillna(df_simple[i].median())\n",
    "\n",
    "df_simple.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97720b08",
   "metadata": {},
   "source": [
    "### Fill small gaps in key categorical columns\n",
    "\n",
    "Some rows still have a few missing labels (NaN) in columns like flight purpose, aircraft damage, and weather.\n",
    "Rather than lose those rows, we replace each missing entry with the **most common value (mode)** for that column.\n",
    "This keeps the categories consistent and avoids introducing new labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll also fill categorical data with few values missing with the most frequent category\n",
    "categorical_data = [\"Purpose.of.flight\", \"Aircraft.damage\", \"Injury.Severity\", \"Engine.Type\", \"Weather.Condition\", \"Amateur.Built\"]\n",
    "\n",
    "for i in categorical_data:\n",
    "    most_common = df_simple[col].mode(dropna=True)[0] # to get the mode\n",
    "    df_simple[i].fillna(most_common, inplace=True)\n",
    "\n",
    "df_simple.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521f84e",
   "metadata": {},
   "source": [
    "### Inspect the raw “Make” values for inconsistencies\n",
    "\n",
    "Before cleaning, we list the 20 most frequent manufacturer names. This quick peek shows issues like mixed casing, extra spaces, and punctuation that we’ll fix next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e20a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a naming inconsistency in the names of makes. We have to resolve that\n",
    "df_simple[\"Make\"].dropna().value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a2077",
   "metadata": {},
   "source": [
    "### Standardize “Make” names and preview common models\n",
    "\n",
    "We clean the **Make** column by capitalizing, trimming spaces, and stripping punctuation so each manufacturer is counted once. After that, we check the 20 most common **Model** names to see what aircraft appear most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcdbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined all names by making case uniform and removing whitespace \n",
    "df_simple[\"Make\"] = df_simple[\"Make\"].str.capitalize().str.strip().str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "df_simple[\"Model\"].dropna().value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c068c5b",
   "metadata": {},
   "source": [
    "### Keep only rows with both Make and Model\n",
    "\n",
    "Rows missing either field can’t be tied to a specific aircraft, so we drop them to ensure every record has a valid manufacturer and model for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly, we drop every row that doesn't have the make or model\n",
    "df_simple = df_simple.dropna(subset=[\"Make\", \"Model\"])\n",
    "df_simple.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325689a",
   "metadata": {},
   "source": [
    "We now have a clean dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
